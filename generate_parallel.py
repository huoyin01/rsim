import argparse
import torch
from tqdm import tqdm
import os
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from modeling_rism import RsimDecoder, RsimForCausalLM
from riscvTokenizer import riscvTokenizer
from utils import COVER_MODULES_RANGE, INSTRUCTION_MAX_LENGTH, check_ins_legal
import time


parser = argparse.ArgumentParser()
parser.add_argument('--model_name_or_path', type=str, required=True)
parser.add_argument('--input_text', type=str, default=None)
# å‡è®¾ INSTRUCTION_MAX_LENGTH é»˜è®¤ä¸º 7
parser.add_argument('--max_length', type=int, default=INSTRUCTION_MAX_LENGTH if 'INSTRUCTION_MAX_LENGTH' in globals() else 7) 
parser.add_argument('--temperature', type=float, default=1.0)
parser.add_argument('--top_k', type=int, default=50)
parser.add_argument('--top_p', type=float, default=1.0)
parser.add_argument('--batch_size', type=int, default=16)
parser.add_argument('--output_dir', type=str, default="generation_parallel")


def tokenize_input(cover_modules_input, batch_size=16, device='cpu'):
    """ç”Ÿæˆ batch_size ä¸ªç›¸åŒè¾“å…¥ï¼Œç”¨äºå¹¶è¡Œ"""
    pairs = cover_modules_input.split(',')
    inputs = [0] * len(COVER_MODULES_RANGE)
    for pair in pairs:
        index, value = pair.split(':')
        inputs[int(index)-1] = int(value)
    tensor = torch.tensor(inputs, device=device, dtype=torch.long)
    tensor = tensor.unsqueeze(0).repeat(batch_size, 1)
    return tensor


def batch_generate(
    model,
    tokenizer,
    cover_inputs,
    max_length,
    temperature,
    top_k,
    top_p,
    batch_size  # <--- å¢åŠ  batch_size å‚æ•°
):
    device = cover_inputs.device
    model.eval()
    
    # æ¯ç»„ç”Ÿæˆ1000æ¡
    steps = 1000

    # <--- æ›´æ”¹ï¼šåœ¨GPUä¸Šé¢„å…ˆåˆ†é…å¥½ (steps, batch_size, max_length) çš„ Tensor
    # æ³¨æ„: model.generate è¿”å› (batch_size, max_length + 1)
    # æˆ‘ä»¬å– [:, 1:]ï¼Œæ‰€ä»¥é•¿åº¦æ˜¯ max_length (ä¾‹å¦‚ 7)
    all_generated_tokens = torch.empty(
        (steps, batch_size, max_length), 
        dtype=torch.long, 
        device=device
    )

    with tqdm(total=steps, desc="Generating batch instructions") as pbar:
        # <--- æ›´æ”¹ï¼šä½¿ç”¨ step_idx æ¥ç´¢å¼•
        for step_idx in range(steps):
            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                generated_tokens, next_cov = model.generate(
                    inputs=torch.full((cover_inputs.size(0), 1), -1, device=device, dtype=torch.long),
                    cover_modules=cover_inputs,
                    max_length=max_length + 1,
                    do_sample=True,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    bos_token_id=tokenizer.bos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                )

            # <--- æ›´æ”¹ï¼šç›´æ¥å¡«å……åˆ°é¢„åˆ†é…çš„Tensorä¸­
            # generated_tokens[:, 1:] çš„å½¢çŠ¶æ˜¯ (batch_size, max_length)
            all_generated_tokens[step_idx] = generated_tokens[:, 1:]

            # ä¸‹ä¸€è½®è¾“å…¥ä½¿ç”¨ next_cov
            cover_inputs = next_cov
            pbar.update(1)

    # <--- æ›´æ”¹ï¼šå¾ªç¯ç»“æŸåï¼Œè¿›è¡Œå˜å½¢å’ŒCPUè½¬ç§»
    
    # å˜å½¢: [steps, batch_size, max_length] -> [batch_size, steps, max_length]
    # (1000, 16, 7) -> (16, 1000, 7)
    all_generated_tokens = all_generated_tokens.permute(1, 0, 2)
    
    # ä¸€æ¬¡æ€§å°†å¤§Tensorè½¬ç§»åˆ°CPU
    all_generated_tokens_cpu = all_generated_tokens.cpu()
    
    print(f"\nâœ… All tokens generated and moved to CPU. Tensor shape: {all_generated_tokens_cpu.shape}")

    return all_generated_tokens_cpu


# <--- å…³é”®ä¿®æ”¹ï¼šå°† check_group ç§»åˆ°é¡¶å±‚ï¼ˆå…¨å±€ä½œç”¨åŸŸï¼‰ ---
def check_group(idx, ins_list_tensor): 
    """
    ä¸€ä¸ªé¡¶å±‚å‡½æ•°ï¼Œç”¨äºè¢«å­è¿›ç¨‹ pickle å’Œè°ƒç”¨ã€‚
    æ£€æŸ¥ä¸€ä¸ª [N, 7] çš„ Tensorï¼Œè¿”å›åˆæ³•çš„æŒ‡ä»¤ã€‚
    """
    legal = []
    # è¿™é‡Œçš„ check_ins_legal å¿…é¡»ä¹Ÿæ˜¯é¡¶å±‚å¯å¯¼å…¥çš„ï¼ˆä½ å·²ç»åšåˆ°äº†ï¼‰
    for ins in ins_list_tensor: 
        if check_ins_legal(ins): 
            legal.append(ins)
    return idx, legal


def filter_illegal_instructions(instructions_tensor):
    """å¯¹ 16 ç»„ç»“æœ (æ¥è‡ªä¸€ä¸ª [16, 1000, 7] çš„Tensor) è¿›è¡Œå¹¶è¡Œéæ³•æ£€æµ‹"""
    
    # <--- æœ¬åœ°å®šä¹‰çš„ check_group å·²è¢«ç§»é™¤ ---

    num_batches = instructions_tensor.size(0) 
    filtered_results = [None] * num_batches
    print(f"Using ProcessPoolExecutor with max_workers= {num_batches}")
    
    with ProcessPoolExecutor(max_workers=256) as executor:
        futures = {
            # <--- å…³é”®ä¿®æ”¹ï¼šç°åœ¨è°ƒç”¨çš„æ˜¯é¡¶å±‚çš„ check_group ---
            executor.submit(check_group, i, instructions_tensor[i]): i 
            for i in range(num_batches)
        }
        
        with tqdm(total=len(futures), desc="Filtering batches (Parallel)") as pbar:
            for future in as_completed(futures):
                idx, legal = future.result()
                filtered_results[idx] = legal
                pbar.update(1)
                
    return filtered_results


# <--- ç§»é™¤ä¸€ä¸ªé‡å¤çš„ ThreadPoolExecutor å¯¼å…¥
# from concurrent.futures import ThreadPoolExecutor, as_completed

def save_results(filtered_results, output_dir):
    """æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹"""
    os.makedirs(output_dir, exist_ok=True)
    tokenizer = riscvTokenizer()  # decode æ—¶éœ€è¦

    print(f"Saving results to {output_dir}...")
    for i, lst in enumerate(filtered_results, start=1):
        with open(os.path.join(output_dir, f"{i}.txt"), "w", encoding="utf-8") as f:
            for ins in lst:
                decoded = tokenizer.decode(ins)
                f.write(decoded.replace(',<pad>', '') + "\n")
    print(f"âœ… Saved {len(filtered_results)} files in {output_dir}/")


def main():
    args = parser.parse_args()
    tokenizer = riscvTokenizer()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = RsimForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32).to(device)
    print(f"Using device: {device}")

    input_text = args.input_text or "1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0,13:0,14:0,15:0,16:0,17:0,18:0,19:0,20:0,21:0,22:0"
    cover_inputs = tokenize_input(input_text, args.batch_size, device=device)

    print(f"ğŸš€ Using batch size = {args.batch_size}")
    start_time = time.time()
    
    # <--- æ›´æ”¹ï¼šä¼ å…¥ args.batch_size å¹¶æ¥æ”¶ tensor
    generated_cpu_tensor = batch_generate(
        model,
        tokenizer,
        cover_inputs,
        args.max_length,
        args.temperature,
        args.top_k,
        args.top_p,
        args.batch_size # <--- ä¼ å…¥ batch_size
    )
    end_time = time.time()
    run_time = end_time - start_time
    print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {run_time:.4f} ç§’")
    print("ğŸ” Filtering illegal instructions...")
    start_time = time.time()
    # <--- æ›´æ”¹ï¼šå°† tensor ä¼ å…¥
    filtered = filter_illegal_instructions(generated_cpu_tensor) 

    save_results(filtered, args.output_dir)
    end_time = time.time()
    run_time = end_time - start_time
    print(f"â±ï¸ å¤„ç†æ—¶é—´: {run_time:.4f} ç§’")


if __name__ == "__main__":
    main()
